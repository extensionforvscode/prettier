{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06bf9e1b-28a8-4306-9b83-a966bf87a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea13612e-fe35-4678-bd45-640e136d37b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>morning</td>\n",
       "      <td>What a great day!!! Looks like dream.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>noon</td>\n",
       "      <td>I feel sorry, I miss you here in the sea beach</td>\n",
       "      <td>positive</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>night</td>\n",
       "      <td>Don't angry me</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>We attend in the class just for listening teac...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>noon</td>\n",
       "      <td>Those who want to go, let them go</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Day Time of Tweet                                               text sentiment     Platform\n",
       "0  2018      8   18       morning              What a great day!!! Looks like dream.  positive    Twitter  \n",
       "1  2018      8   18          noon     I feel sorry, I miss you here in the sea beach  positive    Facebook \n",
       "2  2017      8   18         night                                     Don't angry me  negative     Facebook\n",
       "3  2022      6    8       morning  We attend in the class just for listening teac...  negative    Facebook \n",
       "4  2022      6    8          noon                  Those who want to go, let them go  negative   Instagram "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Manthan\\Desktop\\jupyter_extension_prettier\\vs_code_extension\\sentiment_analysis.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82f4ce17-d7e7-46af-aeda-359eee892529",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "max_vocab = 5000\n",
    "\n",
    "# object of tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_vocab, oov_token=\"OOV\")\n",
    "\n",
    "# shorten the data\n",
    "data = data[[\"text\", \"sentiment\"]]\n",
    "data = data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77abbc0c-7f4f-4f0c-b850-6c2880852c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 68   9  11 306 448   2  51   8 449 450 307   9 233   2  48 451  95  28\n",
      "  80  10 234 452  10  74 235 453   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "data[\"text\"] = data[\"text\"].astype(str).fillna(\"unknown\")\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer.fit_on_texts(data[\"text\"])\n",
    "\n",
    "\n",
    "X=tokenizer.texts_to_sequences(data[\"text\"])\n",
    "X=pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "print(X[98])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "512322c4-3f98-419e-a6c8-521b8f101d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {label: idx for idx, label in enumerate(data[\"sentiment\"].unique())}\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f827e246-6eb1-4e32-ab25-ca028b09a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y = data[\"sentiment\"].map(label_mapping).values\n",
    "print(y[0])\n",
    "y = to_categorical(y)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df55a3a2-4306-427a-821f-1b23196e7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 50)\n",
      "(399, 3)\n",
      "(100, 50)\n",
      "(100, 3)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print((np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07afba1c-fb04-4d21-87f7-55df6a3e5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab, output_dim=128, input_length=128),\n",
    "    LSTM(64),\n",
    "    Dense(y.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6e2d9b-1ad0-44c2-80e3-338724b3891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.3826 - loss: 1.0929\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4319 - loss: 1.0721\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4446 - loss: 1.0703\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4066 - loss: 1.0779\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4292 - loss: 1.0788\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4348 - loss: 1.0710\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4111 - loss: 1.0815\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4378 - loss: 1.0452\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5988 - loss: 0.8126\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6782 - loss: 0.6784\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4760 - loss: 1.1350 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1634173393249512, 0.4399999976158142]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be687a3f-7df3-47ad-8ed0-ecd3c330c5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  6   0   0 ...   0   0   0]\n",
      " ...\n",
      " [193   0   0 ...   0   0   0]\n",
      " [  6   0   0 ...   0   0   0]\n",
      " [950   0   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(\"I am very happing today\")\n",
    "test_seq = pad_sequences(test_seq, maxlen=max_len, padding=\"post\")\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9aa103c9-da9c-478a-a0c2-34316ae851d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predicted sentiment index: 2\n"
     ]
    }
   ],
   "source": [
    "sentiment = model.predict(test_seq)\n",
    "sentiment_label = sentiment.argmax(axis=-1)\n",
    "print(f\"Predicted sentiment index: {sentiment_label[0]}\")\n",
    "\n",
    "# {'positive': 0, 'negative': 1, 'neutral': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a48f4-b78b-4524-82ea-e2a12a5481c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
